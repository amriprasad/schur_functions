\documentclass[11pt]{amsart}
\usepackage{ytableau,graphicx, xypic}
\xyoption{all}
\title{An Introduction to Schur Functions}
\author{Amritanshu Prasad}
\newtheorem{theorem}{Theorem}[subsection]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{example}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\renewcommand{\thesubsection}{\arabic{subsection}}
\newcommand{\ev}{\textup{ev}}
\newcommand{\rins}{\iota}
\newcommand{\ins}{\textup{INSERT}}
\newcommand{\del}{\textup{DELETE}}
\newcommand{\rdel}{\delta}
\newcommand{\pl}{\textup{Pl}}
\newcommand{\supp}{\textup{supp}}
\newcommand{\wt}{\textup{wt}}
\newcommand{\shape}{\textup{shape}}
\newcommand{\abc}{\textup{Abc}}
\newcommand{\Tab}{\textup{Tab}}
\newcommand{\rsk}{\textup{RSK}}
\newcommand{\bur}{\textup{BUR}}
\renewcommand{\circled}[1]{\raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {#1}}}}
\DeclareMathOperator{\tab}{Tab}
\begin{document}
\maketitle
\subsection{Symmetric Polynomials}
\label{sec:symmetric-functions}
Consider polynomials in $n$ variables \linebreak $x_1,\dotsc,x_n$.
Given a multiindex $\alpha=(\alpha_1,\dotsc, \alpha_n)$, let $x^\alpha$ denote the monomial $x_1^{\alpha_1}\dotsb x_n^{\alpha_n}$.
A \emph{symmetric polynomial} is a polynomial of the form
\begin{displaymath}
  f(x_1,\dotsc, x_n) = \sum_{\alpha} c_\alpha x^\alpha,
\end{displaymath}
where, for any permutation $w\in S_n$,
\begin{displaymath}
  c_{(\alpha_1,\dotsc,\alpha_n)} = c_{(\alpha_{w(1)},\dotsc,\alpha_{w(n)})}.
\end{displaymath}
The integer partition $\lambda$ obtained by sorting the coordinates of $\alpha$  is called the \emph{shape} of $\alpha$, denoted $\lambda(\alpha)$.
The most obvious example of a symmetric polynomial in $n$ variables is the \emph{monomial symmetric function}, defined for each integer partition $\lambda$:
\begin{displaymath}
  m_\lambda = \sum_{\lambda(\alpha) = \lambda} x^\alpha.
\end{displaymath}
Note that $m_\lambda$ is homogeneous of degree $|\lambda|$ (the sum of the parts of $\lambda$).
\begin{exercise}
  Take $n=4$. Compute the monomial symmetric functions $m_{(3)}$, $m_{(2,1)}$, and $m_{(1^3)}$.
\end{exercise}
\begin{theorem}
The polynomials $m_\lambda(x_1,\dotsc,x_n)$, as $\lambda$ runs over all the integer partition of $d$, form a basis for the space of homogeneous symmetric polynomials of degree $d$ in $n$ variables.
\end{theorem}
\subsection{Complete and Elementary Symmetric Polynomials}
\label{sec:compl-elem-symm}
Recall that the coefficients of a polynomial are symmetric polynomials in its roots:
\begin{multline}
  \label{eq:elem-id}
  (t-x_1)(t-x_2)\dotsb (t-x_n) \\= t^n - e_1(x_1,\dotsc, x_n)t^{n-1} + \dotsb + (-1)^n e_n(x_1,\dotsc, x_n),
\end{multline}
where coefficient $e_i(x_1,\dotsc, x_n)$ of $t^{n-i}$ is given by:
\begin{equation}
  \label{eq:elem}
  e_i(x_1,\dotsc, x_n) = \sum_{1\leq j_1<\dotsb<j_i\leq n} x_{j_1}x_{j_2}\dotsb x_{j_i}.
\end{equation}
The polynomial $e_i$ is called the $i$th \emph{elementary symmetric polynomial}.
By convention, $e_i(x_1,\dotsc,x_n)=0$, for $i>n$.

The identity (\ref{eq:elem-id}) can be written more elegantly as:
\begin{displaymath}
  (1+t x_1) \dotsb (1+tx_n) = \sum_{i=0}^\infty e_i(x_1,\dotsb, x_n)t^i.
\end{displaymath}

Dually\footnote{We will refer to the replacing of $(1+u)$ by $(1-u)^{-1}$ in a formal identity as \emph{dualization}.}, the \emph{complete symmetric polynomials} are defined by the formal identity:
\begin{displaymath}
  \frac 1{(1-x_1t)\dotsb (1-x_nt)} = \sum_{i=0}^\infty h_i(x_1,\dotsb, x_n)t^i.
\end{displaymath}
\begin{example}
  In three variables:
  \begin{align*}
    e_2(x_1,x_2,x_3) & = x_1x_2 + x_1x_3 + x_2x_3,\\
    h_2(x_1,x_2,x_3) & = x_1^2 + x_1x_2 + x_1x_3 + x_2^2 + x_2x_3 + x_2^3.
  \end{align*}
\end{example}
\begin{exercise}
  Show that
  \begin{displaymath}
    h_i(x_1,\dotsb x_n) = \sum_{1\leq j_1\leq \dotsb \leq j_i\leq n} x_{j_1}\dotsb x_{j_i}.
  \end{displaymath}
  and that
  \begin{displaymath}
    e_i(x_1,\dotsb x_n) = \sum_{1\leq j_1< \dotsb < j_i\leq n} x_{j_1}\dotsb x_{j_i}.
  \end{displaymath}
\end{exercise}
More generally, for any integer partition $\lambda=(\lambda_1,\dotsc, \lambda_l)$, define:
\begin{align*}
  h_\lambda &= h_{\lambda_1} h_{\lambda_2}\dotsb h_{\lambda_l},\\
  e_\lambda &= e_{\lambda_1} e_{\lambda_2}\dotsb e_{\lambda_l}.
\end{align*}
\begin{theorem}
  Given partitions $\lambda=(\lambda_1,\dotsc,\lambda_l)$ and $\mu=(\mu_1,\dotsb, \mu_m)$ of $d$, let $M_{\lambda\mu}$ denote the number of matrices $(a_{ij})$ with non-negative integer entries whose $i$th row sums to $\lambda_i$ for each $i$, and whose $j$th column sums to $\mu_j$ for each $j$.
  Then
  \begin{displaymath}
    h_\lambda = \sum_\mu M_{\lambda\mu} m_\mu.
  \end{displaymath}
  Dually, let $=N_{\lambda\mu}$ denote the number of integer matrices $(a_{ij})$ with entries $0$ or $1$, whose $i$th row sums to $\lambda_i$ for each $i$, and whose $j$th column sums to $\mu_j$ for each $j$.
  \begin{displaymath}
    e_\lambda = \sum_\mu N_{\lambda\mu} m_\mu.
  \end{displaymath}
\end{theorem}
\begin{proof}
  To prove the second identity involving elementary symmetric functions, note that a monomial in the expansion of
  \begin{displaymath}
    e_\lambda = \prod_{i=1}^l \sum_{j_1<\dotsb<j_{\lambda_j}}x_{j_1}\dotsb x_{j_{\lambda_i}}
  \end{displaymath}
  is a product of summands, one chosen from each of the $l$ factors.
  Construct an $l\times m$ matrix $(a_{ij})$ corresponding to such a choice as follows:
  if the summand $x_{j_1}\dotsb x_{j_{\lambda_i}}$ is chosen from the $i$th factor, then set the entries $a_{i,j_1},\dotsc, a_{i, j_{\lambda_j}}$ to be $1$ (the remaining entries of the $i$th row are $0$).
  Clearly the $i$th row of such a matrix sums to $\lambda_i$.
  The monomial corresponding to this choice is $x^\mu$ if, for each $j$, the the number of $i$ for which $x_j$ appears in the monomial corresponding to the $i$gth row is $\mu_j$. This is just the sum of the $j$th column of the matrix $(a_{ij})$.
  It follows that the coefficient of $x^\mu$, and hence the coefficient of $m_\mu$ in the expansion of $e_\lambda$ in the basis of monomial symmetric functions of degree $n$, is $N_{\lambda\mu}$.

A similar proof can be given for the first identity involving complete symmetric functions. The only difference is that variables may be repeated in the monomials that appear in $h_i$. Counting the number of repetitions (instead of just recording $0$ or $1$) gives non-negative integer matrices.
\end{proof}
\subsection{Alternating Polynomials}
\label{sec:alt-poly}
An \emph{alternating polynomial} in $x_1,\dotsc, x_n$ is of the form:
\begin{equation}
  \label{eq:alt-form}
  f(x_1,\dotsc,x_n) = \sum_{\alpha} c_\alpha x_\alpha,
\end{equation}
where, $c_{w(\alpha)} = \epsilon(w)c_\alpha$ for every multiindex $\alpha$ as in Section~\ref{sec:symmetric-functions}.
Here $\epsilon:S_n\to \{\pm 1\}$ denotes the sign function.
Equivalently, an alternating polynomial is one whose sign is reversed upon the interchange of any two variables.
\begin{exercise}
  If $\alpha$ is a multiindex where $\alpha_i=\alpha_j$ for some $i\neq j$, then $c_\alpha = 0$.
\end{exercise}
In particular, every monomial in an alternating polynomial must be composed of distinct powers.
Moreover, the polynomial is completely determined by the coefficients with strictly decreasing multiindices, namely, multiindices of the form $c_\alpha$, where $\alpha=(\alpha_1,\dotsc,\alpha_n)$ with $\alpha_1>\dotsb>\alpha_n$.
\begin{exercise}
  Let $\delta=(n-1,n-2,\dotsc,1, 0)$.
  Given an integer partition with at most $n$ parts, we will pad it with $0$'s so that it can be regarded as a weakly decreasing multiindex of length $n$.
  Then $\lambda\mapsto \lambda+\delta$ is a bijection from the set of integer partitions with at most $n$ onto the set of strictly decreasing multiindices.
\end{exercise}
\begin{example}
  Let $\lambda = (\lambda_1,\dotsc, \lambda_n)$ be a weakly decreasing multiindex.
  The polynomial:
  \begin{displaymath}
    a_{\lambda+\delta} = \det(x_i^{\lambda_j + n - j})
  \end{displaymath}
  is alternating, with unique strictly decreasing monomial $x^{\lambda+\delta}$.
\end{example}
\begin{exercise}
  \label{exercise:alt-basis}
  The alternating polynomial of the form \textup{(\ref{eq:alt-form})} is equal to  \begin{displaymath}
    \sum_{\lambda} c_\lambda a_{\lambda+\delta},
  \end{displaymath}
  the sum being over all weakly decreasing multiindices $\lambda$.
\end{exercise}
\subsection{Interpretation of Alternants with Labeled Abaci}
\label{sec:abaci}
A labeled abacus with $n$ beads is a word $w=(w_k; k\geq 0)$ such that the subword of non-zero letters is a permutation of $1,2,\dotsc,n$.
The sign $\epsilon(w)$ of the abacus is the sign of this permutation, the support is the set $\supp(w)=\{k\mid w_k>0\}$, and the weight is defined as:
\begin{displaymath}
  \wt(w) = \prod_k x_{w_k}^k.
\end{displaymath}
The shape of the abacus, $\shape(w)$ is the unique partition $\lambda$ such that the components of $\lambda+\delta$ form the support of $w$.
\begin{example}
  Consider the labeled abacus $w=510032046000\dotsb$.
  Its underlying permutation is $513246$, which has sign $-1$, so $\epsilon(w)=-1$.
  Also, $\supp(w)=\{0,1,4,5,7,8\}$, $\shape(w)=(3,3,2,2)$ (indeed, $(3,3,2,2,0,0)+(5,4,3,2,1,0)=(8,7,5,4,1,0)$) and $\wt(w)=x_5^0x_1^1x_3^4x_2^5x_4^7x_6^8$.
  We visualize the abacus $w$ as a configuration of beads on a single runner, with possible positions of beads numbered $1, 2, 3, \dotsc$.
  If $w_k=i$ where $i>0$, then a bead labeled $i$ is placed in position $k$ on the runner.
  If $w_k=0$, then the position $k$ is unoccupied.
  In the running example the visualization is:
  \begin{displaymath}
    \begin{tabular}{*{10}{p{0.7cm}}}
      0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9\\
      \circled{5} & \circled{1} & $\bullet$ & $\bullet$ & \circled{3} & \circled{2} & $\bullet$ & \circled{4} & \circled{6} & $\bullet$
    \end{tabular}
  \end{displaymath}
  The first row shows the positions $k=0,1,\dotsc$ on the runner and the second row shows the beads.
\end{example}
\begin{theorem}
  \label{theorem:abacus-alt}
  For every partition $\lambda$ the alternant in $n$  variables,
  \begin{displaymath}
    a_{\lambda+\delta} = (-1)^{\lfloor n/2\rfloor} \sum_w \epsilon(w)\wt(w),
  \end{displaymath}
  the sum being over all labeled abaci with $n$ beads and shape $\lambda$.
\end{theorem}
\begin{proof}
  The theorem follows from the expansion of the determinant.
\end{proof}
\subsection{Cauchy's Bialternant Form of a Schur Function}
\label{sec:cauchys-bialt-form}
The simplest polynomial of the form $a_{\lambda+\delta}$ arises when $\lambda=0$; $a_\delta$ is the Vandermonde determinant:
\begin{displaymath}
  a_\delta = \prod_{1\leq i<j\leq n}(x_i-x_j).
\end{displaymath}
\begin{exercise}
  Show that, for every weakly decreasing multiindex $\lambda$, $a_{\lambda+\delta}$ is divisible by $a_\delta$ in the ring of polynomials in $x_1,\dotsc,x_n$.
\end{exercise}
\begin{exercise}
  \label{exercise:vandermonde-iso}
  Show that $f\mapsto fa_\delta$ is an isomorphism of the space of symmetric polynomials in $x_1,\dotsc, x_n$ of degree $d$ onto the space of alternating polynomials of degree $d + \binom n2$.
\end{exercise}
The above exercise gives the historically oldest definition of Schur functions---\emph{Cauchy's bialternant formula}:
\begin{equation}
  \label{eq:schur}
  s_\lambda(x_1,\dotsc,x_n) = a_{\lambda+\delta}/a_\delta,
\end{equation}
for any partition $\lambda$ with at most $n$ parts.
If $\lambda$ has more than $n$ parts, set $s_\lambda(x_1,\dotsc,x_n) =0$.
This is clearly a symmetric function of degree $|\lambda|$.
\begin{theorem}
  As $\lambda$ runs over all integer partitions of $d$ with at most $n$ parts, the Schur functions $s_\lambda(x_1,\dotsc,x_n)$ form a basis of the space of all homogeneous symmetric functions in $x_1,\dotsc,x_n$ of degree $d$.
\end{theorem}
\begin{proof}
  This follows from Exercises~\ref{exercise:alt-basis} and~\ref{exercise:vandermonde-iso}.
\end{proof}
\begin{exercise}
  [Stability of Schur functions]
  Show that substituting $x_n=0$ in the Schur function $s_\lambda(x_1,\dotsc, x_n)$ with $n$ variables gives the corresponding Schur function $s_\lambda(x_1,\dotsc,x_{n-1})$ with $n-1$ variables.
\end{exercise}
\subsection{Pieri's rule}
\label{sec:pieri}
The set of integer partitions is endowed with the \emph{containment order}.
We say that a partition $\lambda=(\lambda_1,\dotsc,\lambda_l)$ \emph{contains} a partition $\mu=(\mu_1,\dotsc, \mu_m)$ if $l \geq m$, and $\lambda_i\geq \mu_i$ for every $i=1,\dotsb, m$.
We write $\lambda\supset\mu$ or $\mu \subset \lambda$.
Recall that the Young diagram of the partition $\lambda$ is the set of points 
\begin{displaymath}
\{(i, j)\mid 1\leq i\leq l,\; 1\leq j\leq \lambda_i\}.
\end{displaymath}
Visually, each node $(i,j)$ of the Young diagram is replaced by a box, and the box corresponding to $(i,j)$ is placed in the $i$th row and $j$th column (matrix notation).
Thus, the Young diagram of $\lambda=(6, 5, 3, 3)$ is depicted by:
\ytableausetup{smalltableaux}
\begin{displaymath}
  \ydiagram{6,5,3,3}
\end{displaymath}
Note that containment of partitions is nothing but the containment relation on their Young diagrams.
By abuse of notation, we will also use $\lambda$ to denote the Young diagram of $\lambda$.

By a skew-shape, we mean a difference of Young diagrams $\lambda \setminus \mu$, where $\lambda \supset \mu$.
We write $\lambda/\mu$ for this skew-shape.
A skew-shape is called a \emph{horizontal strip} (respectively, a \emph{vertical strip}) if it has at most one box in each vertical column (respectively, horizontal row).
\begin{theorem}
  For every partition $\lambda$, and every positive integer $k$,
  \begin{displaymath}
    s_\lambda h_k = \sum_\mu s_\mu,
  \end{displaymath}
  where the sum runs over all partitions $\mu\supset\lambda$ such that $\mu/\lambda$ is a horizontal strip of size $k$.
  Dually,
  \begin{displaymath}
    s_\lambda e_k = \sum_\mu s_\mu,
  \end{displaymath}
  where the sum runs over all partitions $\mu\supset\lambda$ such that $\mu/\lambda$ is a vertical strip of size $k$.
\end{theorem}
\begin{proof}
  We reproduce the proof from Loehr \cite{loehr}. 
  Let $\abc(\lambda)$ denote the set of all $n$-bead labeled abaci (see Section~\ref{sec:abaci}) of shape $\lambda$.
  Let $M(n,k)$ denote the set of all vectors $\alpha=(\alpha_1,\dots,\alpha_n)$ with non-negative integer coordinates and sum $k$.
  Set $\wt(\alpha)=x_1^{\alpha_1}\dotsb x_n^{\alpha_n}$.
  The first identity is equivalent to showing that:
  \begin{displaymath}
    \sum_{w\in \abc(\lambda)} \epsilon(w)\sum_{\alpha\in M(n,k)} \wt(\alpha) = \sum_\mu \sum_{w\in \abc(\mu)} \epsilon(w) \wt(w),
  \end{displaymath}
  the sum on the right being over all partitions $\mu\supset\lambda$ such that $\mu/\lambda$ is a horizontal strip.
  We will define an involution $I$ on the $\abc(\lambda)\times M(n,k)$ whose fixed points correspond to elements of $\coprod_{\mu/\lambda \text{ is a horiz. strip of size $k$}}\abc(\mu)\times M(n,k)$ under a bijection that preserves weights and signs, and such that if $I(w, \alpha)=(w', \alpha')$ then $\wt(w)\wt(\alpha)=\wt(w')\wt(\alpha')$ and $\epsilon(w') = -\epsilon(w)$.
  Then all terms on the left hand side, except for those which do not correspond to fixed points, will cancel, and the surviving terms will give the right hand side.

  To construct $I$, scan the abacus from left to right.
  Upon encountering a bead numbered $j$, move the bead $\alpha_j$ steps to the right, one step at a time.
  If this process completes without this bead colliding with another bead, $(w,\alpha)$ is a fixed point of $I$.
  The new abacus $w^*$ has $\epsilon(w^*)=\epsilon(w)$ (the underlying permutation remains unchanged), and $\shape(w^*)/shape(w)$ is a horizontal strip of size $k$.

  However, suppose a collision does occur, say the first collision is when bead $j$ hits bead $k$ that is located $p\leq \alpha_j$ position to the right of its initial position.
  Define $I(w,\alpha) = (w',\alpha')$, where $w'$ is $w$ with the beads $i$ and $j$ interchanged, $\alpha'_j=\alpha_j-p$, $\alpha'_k=\alpha_k+p$ and all other coordinates of $\alpha$ and $\alpha'$ are equal.
  Clearly $w'$ has the opposite sign from $w$, and $\wt(w)\wt(\alpha)=\wt(w')\wt(\alpha')$.
  One checks that $I(w',\alpha')=(w,\alpha)$.
  \begin{example}
    \label{example:bead-h}
    Let $n=6$, $\lambda=(3,3,2,2,0,0)$, $k=3$, and
    \begin{displaymath}
      (w, \alpha) = (51003204600\dotsb, (2, 1, 0, 0, 0, 0)). 
    \end{displaymath}
    Reading the abacus from left to right, the first bead encountered is numbered $1$, which can be moved $2$ places to the right without any collisions.
    After that the bead numbered $2$ can be moved $1$ place to the right, again without collisions.
    So $(w,\alpha)$ is a fixed point for $I$.
    The new abacus $50013024600\dotsb$ has shape $(3,3,3,2,2,0)$ obtained by adding a horizontal $3$-strip to $(3, 3, 2, 2, 0, 0)$.

    On the other hand, if $\alpha=(1, 1, 1, 0, 0, 0)$, then the first collision is of the bead numbered $3$ with the bead numbered $2$ in the very first step.
    We have $I(w,\alpha) = (51002304600\dotsb, (1,2,0,0,0,0))$.
  \end{example}

  Let $N(n,k)$ denote the set of vectors $\alpha=(\alpha_1,\dotsc,\alpha_n)$ such that $\alpha_i\in \{0,1\}$ for each $i$, and $\alpha_1+\dotsb+\alpha_n=k$.
  For elementary symmetric functions, we wish to prove:
  \begin{displaymath}
    \sum_{w\in \abc(\lambda)} \epsilon(w)\sum_{\alpha\in N(n,k)} \wt(\alpha) = \sum_\mu \sum_{w\in \abc(\mu)} \epsilon(w) \wt(w),
  \end{displaymath}
  where $\mu$ runs over all partition such that $\mu/\lambda$ is a vertical strip of size $k$.
  
  We construct an involution $I$ on $\abc(\lambda)\times N(n,k)$ as follows: scan the abacus from \emph{right to left}.
  Upon encountering a bead numbered $j$, if $\alpha_j=1$, try to move the bead one step to the right.
  If this process completes without collisions, then $(w,\alpha)$ is a fixed point of $I$.
  Otherwise, if the first collision occurs with bead numbered $j$ colliding with bead numbered $k$, then define $w'$ to be $w$ with beads $j$ and $k$ interchanged.
  Also, since the $k$th bead was adjacent to the $j$th bead, it could not have been moved in its turn.
  So $\alpha_k=0$.
  Let $\alpha'$ be obtained from $\alpha$ by interchanging $\alpha_k$ and $\alpha_j$.
\end{proof}
  \begin{example}
  The pair $(51003204600\dotsb,(1,1,1,0,0,0))$ is a fixed point for $I$, and the shifted abacus is $(50100324600\dotsb)$ of shape $(3,3,3,3,1,0)$.
  On the other hand 
  \begin{displaymath}
    I(51003204600\dotsb,(0,0,1,0,1,1))=(51002304600\dotsb, (0,1,0,0,1,1)).
  \end{displaymath}
\end{example}
The following is a special case of Pieri's rule:
\begin{corollary}
  For every positive integer $k$,
  \begin{displaymath}
    s_{(k)} = h_k, \text{ and } s_{(1^k)} = e_k.
  \end{displaymath}
\end{corollary}
\begin{exercise}
  \label{exercise:hook-schur}
  Use Pieri's rule to show that:
  \begin{displaymath}
    h_ke_l = s_{(k, 1^l)} + s_{(k+1, 1^{l-1})}.
  \end{displaymath}
  Conclude that
  \begin{displaymath}
    s_{(j+1,1^k)} = \sum_{l=0}^k (-1)^l h_{j+l+1}e_{k-l}.
  \end{displaymath}
\end{exercise}
\subsection{Schur to Complete and Elementary via Tableaux}
Pieri's rule allows us to compute the complete and elementary symmetric functions $h_\lambda$ and $e_\lambda$ in terms of Schur functions.
\begin{example}
  \label{example:expansion-e}
  Repeated application of Pieri's rule gives an expansion of $e_{(2, 2, 1)} = e_2e_2e_1$ as:
  \begin{displaymath}
    \resizebox{\textwidth}{!}{
      \xymatrix@R-15pt{
        &&&e_2 e_2 e_1\ar[d]&&&\\
        &&&s_{(1^2)} e_2 e_1\ar[dll]\ar[d]\ar[drr]&&\\
        &s_{(2^2)}e_1\ar[dl]\ar[d] && s_{(2, 1^2)}e_1\ar[dl]\ar[d]\ar[dr] && s_{(1^4)}e_1 \ar[d]\ar[dr]&\\
        s_{(3, 2)} & s_{(2^2, 1)} & s_{(3, 1^2)} & s_{(2^2, 1)} & s_{(2, 1^3)} & s_{(2, 1^3)} & s_{(1^5)}
      }
    }
  \end{displaymath}
  giving:
  \begin{displaymath}
    e_{(2^2,1)} = s_{(3,2)} + 2s_{(2^2,1)} + s_{(3,1^2)} + 2s_{(2,1^3)} + s_{(1^5)}.
  \end{displaymath}
  The steps going from the first line of the above calculation to each term of the last line can be recorded by putting numbers into Young diagrams:
  \begin{displaymath}
    \resizebox{0.75\textwidth}{!}{
      \xymatrix@R-15pt{
        &&&\emptyset\ar[d]&&&\\
        &&&\ytableaushort{1,1}\ar[dll]\ar[d]\ar[drr]&&\\
        &\ytableaushort{12,12}\ar[dl]\ar[d] && \ytableaushort{12,1,2}\ar[dl]\ar[d]\ar[dr] && \ytableaushort{1,1,2,2}\ar[d]\ar[dr]&\\
        \ytableaushort{123,12} & \ytableaushort{12,12,3} & \ytableaushort{123,1,2} & \ytableaushort{12,13,2} & \ytableaushort{12,1,2,3} & \ytableaushort{13,1,2,2} & \ytableaushort{1,1,2,2,3}
      }
    }
  \end{displaymath}
  The boxes in the vertical strip added at the $i$th stage are filled with $i$.
\end{example}
\begin{example}
  \label{example:expansion-h}
  Repeated application of Pieri's rule gives an expansion of $h_{(2, 2, 1)} = h_2h_2h_1$ as:
  \begin{displaymath}
    \resizebox{\textwidth}{!}{
      \xymatrix@R-15pt{
        &&&h_2 h_2 h_1\ar[d]&&&\\
        &&&s_{(2)} h_2 h_1\ar[dll]\ar[d]\ar[drr]&&\\
        &s_{(4)}h_1\ar[dl]\ar[d] && s_{(3,1)}h_1\ar[dl]\ar[d]\ar[dr] && s_{(3,2)}h_1 \ar[d]\ar[dr]&\\
        h_{(5)} & s_{(4, 1)} & s_{(4,1)} & s_{(3, 2)} & s_{(3, 1^2)} & s_{(3,2)} & s_{(2,2,1)}
      }
    }
  \end{displaymath}
  giving:
  \begin{displaymath}
    h_{(2^2,1)} = s_{(5)} + 2s_{(4,1)} + 2s_{(3,2)} + s_{(3,1^2)} + s_{(2,2,1)}.
  \end{displaymath}
  The steps going from the first line of the above calculation to each term of the last line can be recorded by putting numbers into Young diagrams:
  \ytableausetup{smalltableaux}
  \begin{displaymath}
    \resizebox{\textwidth}{!}{
      \xymatrix@R-15pt{
        &&&\emptyset\ar[d]&&&\\
        &&&\ytableaushort{11}\ar[dll]\ar[d]\ar[drr]&&\\
        &\ytableaushort{1122}\ar[dl]\ar[d] && \ytableaushort{112,2}\ar[dl]\ar[d]\ar[dr] && \ytableaushort{11,22}\ar[d]\ar[dr]&\\
        \ytableaushort{11223} & \ytableaushort{1122,3} & \ytableaushort{1123,2} & \ytableaushort{112,23} & \ytableaushort{112,2,3} & \ytableaushort{113,22} & \ytableaushort{11,22,3}
      }
    }
  \end{displaymath}
  The boxes in the horizontal strip added at the $i$th stage are filled with $i$.
\end{example}
\begin{definition}
  [Semistandard tableau]
  A semistandard tableau of shape $\lambda=(\lambda_1,\dotsc,\lambda_l)$ and type $\mu=(\mu_1,\dotsc,\mu_m)$ is the Young diagram of $\lambda$ filled with numbers $1,\dotsc, m$ such that the number $i$ appears $\mu_i$ times, the numbers weakly increase along rows, and strictly increase along columns.
\end{definition}
\begin{exercise}
  Semistandard tableaux of shape $\lambda$ and type $\mu$ correspond to chains of integer partitions
  \begin{displaymath}
    \emptyset = \lambda^{(0)} \subset \lambda^{(1)}\subset \lambda^{(2)} \subset \dotsb \subset \lambda^{(m)} = \lambda
  \end{displaymath}
  where $\lambda^{(i)}/\lambda^{(i-1)}$ is a horizontal strip of size $\mu_i$.
\end{exercise}
\begin{example}
  The semistandard tableau of type $(3,2)$ and type $(2,2,1)$ are $\ytableaushort{112,23}$ and $\ytableaushort{113,22}$.
  They correspond to the chains:
  \begin{displaymath}
    \ydiagram{2}\subset \ydiagram{3,1}\subset \ydiagram{3,2} \text{ and } \ydiagram{2}\subset \ydiagram{2,2}\subset \ydiagram{3,2},
  \end{displaymath}
  respectively.
  As illustrated in Example~\ref{example:expansion-h}, the coefficient of $s_{(3,2)}$ in the complete symmetric function $h_{(2,2,1)}$ is the number of semistandard tableau of shape $(3,2)$ and type $(2,2,1)$.
\end{example}
\begin{definition}
  [Kostka number]
  Given two partitions $\lambda$ and $\mu$, the Kostka number $K_{\lambda\mu}$ is the number of semistandard tableau of shape $\lambda$ and type $\mu$.
\end{definition}
\begin{exercise}
  \label{exercise:unit-kostka}
  For every partition $\lambda$, show that $K_{\lambda\lambda}=1$.
\end{exercise}
\begin{exercise}
  [$f$-number]
  The $f$-number of a partition $\lambda$ of $n$ is defined to be the Kostka number $K_{\lambda,(1^n)}$, and is denoted $f_\lambda$.
\end{exercise}
\begin{exercise}
  For a partition $\lambda$, let $\lambda^-$ denote the set of all partitions whose Young diagram can be obtained by removing one box from the Young diagram of $\lambda$.
  Show that $f_\lambda = \sum_{\mu\in \lambda^-} f_\mu$.
\end{exercise}
\begin{exercise}
  A hook is a partition of the form $h(a,b)=(a+1,1^b)$.
  Show that $f_{h(a,b)}=\binom{a+b}a$.
\end{exercise}
In order to understand the expansion of elementary symmetric functions we would need a variant of semistandard tableaux, one where the difference between successive shapes are vertical strips, rather than horizontal strips.
However, it has become common practice to \emph{conjugate} partitions instead:
\begin{definition}
  [Conjugate of a partition]
  The \emph{conjugate} of a partition $\lambda$ is the partition $\lambda'$ whose Young diagram is given by:
  \begin{displaymath}
    \lambda' = \{(j,i)\mid (i, j)\in \lambda\}.
  \end{displaymath}
  In other words, the Young diagram of $\lambda'$ is the reflection of the Young diagram of $\lambda$ about its principal diagonal.
\end{definition}
Clearly $\lambda\mapsto\lambda'$ is an involution.
For example, if $\lambda=(2,2,1)$, then $\lambda'=(3,2)$.
\begin{exercise}
Semistandard tableaux of shape $\lambda'$ and type $\mu$ correspond to chains of integer partitions
  \begin{displaymath}
    \emptyset = \lambda^{(0)} \subset \lambda^{(1)}\subset \lambda^{(2)} \subset \dotsb \subset \lambda^{(m)} = \lambda
  \end{displaymath}
  where $\lambda^{(i)}/\lambda^{(i-1)}$ is a \emph{vertical} strip of size $\mu_i$.  
\end{exercise}
\begin{theorem}
  \label{theorem:schur-to-eh}
  The expansion of complete symmetric functions in terms of Schur functions is given by:
  \begin{displaymath}
    h_\mu = \sum_\lambda K_{\lambda\mu}s_\lambda.
  \end{displaymath}
  Dually, the extension of elementary symmetric functions in terms of Schur functions is given by:
  \begin{displaymath}
    e_\mu = \sum_\lambda K_{\lambda'\mu}s_\lambda.
  \end{displaymath}
\end{theorem}
\subsection{Triangularity of Kostka Numbers}
\label{sec:triang-kostka-numb}
In this section we give a necessary and sufficient condition for the positivity of Kostka number.
As usual, take $\lambda=(\lambda_1,\dotsc,\lambda_l)$ and $\mu=(\mu_1,\dotsc,\mu_m)$.
Suppose $K_{\lambda\mu}>0$.
Then there exists a semistandard tableau $t$ of shape $\lambda$ and type $\mu$.
Since the columns of $t$ are strictly increasing, all the $1$'s in $t$ must occur in its first row, so $\lambda_1\geq \mu_1$.
Also, all the $2$'s must occur in the first two rows (along with all the $1$'s), so $\lambda_1+\lambda_2\geq \mu_1+\mu_2$.
More generally, all the numbers $1,\dotsc, i$ for $i=1,\dotsc,m$ should occur in the first $m$ rows of $t$.
We have:
\begin{equation}
  \label{eq:dominance}
  \lambda_1+\dotsb + \lambda_i \geq \mu_1+\dotsb \mu_i \text{ for } i=1,\dotsc, m.
\end{equation}
\begin{definition}
  We say that an integer partition $\lambda=(\lambda_1,\dotsc,\lambda_l)$ \emph{dominates} the integer partition $\mu=(\mu_1,\dotsc,\mu_m)$ $|\lambda|=|\mu|$ and if the condition (\ref{eq:dominance}) holds.
  When this happens we write $\lambda\rhd \mu$.
  This relation defines a partial order on the set of all integer partitions of $n$ for any non-negative integer $n$.
\end{definition}
\begin{exercise}
  Show that $(n)$ is maximal and $(1^n)$ is minimal among all the integer partitions of $n$.
  What is the smallest integer $n$ for which the dominance order on partitions of $n$ is not a linear order?
\end{exercise}
\begin{theorem}
  [Triangularity of Kostka Numbers]
  \label{theorem:Kostka-triangularity}
  Given partition $\lambda$ and $\mu$ of an integer $n$, $K_{\lambda\mu}>0$ if and only if $\lambda\rhd\mu$.
\end{theorem}
\begin{proof}
  We have already seen that if $K_{\lambda\mu}>0$, then $\lambda \rhd \mu$.
  While reading the proof of the converse, it is helpful to keep in mind Example~\ref{example:can-tab} below.
  Suppose that $\lambda\rhd\mu$.
  Then $\lambda_1\geq \mu_1\geq \mu_m$.
  Therefore, the Young diagram of $\lambda$ has at least $\mu_m$ cells in its first row, or in other words, it has at least $\mu_m$ columns.
  Choose the smallest integer $i$ for which $\lambda_i\geq\mu_m$.
  Fill the bottom-most box in the $\lambda_{i+1}$ leftmost columns with $m$.
  Also, from the $i$th row, fill the rightmost $\mu_m-\lambda_{i+1}$ boxes with $m$.
  The remaining (unfilled) boxes in the Young diagram of $\lambda$ now form the Young diagram of the partition
  \begin{displaymath}
  \eta=(\lambda_1,\dotsc,\lambda_{i-1}, \lambda_i - \mu_m + \lambda_{i+1}, \lambda_{i+2},\dotsc,\lambda_l), 
  \end{displaymath}
  a partition with $l-1$ parts.
  Writing $(\eta_1,\dotsc,\eta_{l-1})$ for the parts of $\eta$, note that, since the first $i-1$ parts of $\eta$ are the same as those of $\lambda$,
  we have:
  \begin{displaymath}
    \eta_1+\dotsb + \eta_j\geq \mu_1 + \dotsb + \mu_j
  \end{displaymath}
  for $j\leq i-1$.
  For $j\geq i$, we have
  \begin{align*}
    \eta_1 + \dotsb + \eta_j & = \lambda_1 + \dotsb + \lambda_{j+1} -\mu_m\\
                             & \geq \mu_1 + \dotsb + \mu_j + \mu_{j+1} - \mu_m\\
                             & \geq \mu_1 + \dotsb + \mu_j.
  \end{align*}
  It follows that $\eta\rhd (\mu_1,\dotsc,\mu_{m-1})$.
  The result now follows by induction in $m$.
\end{proof}
\begin{example}
  \label{example:can-tab}
  Consider the case where $\lambda=(7,3,2)$ and $\mu=(4,4,4)$.
  Then the smallest integer $i$ such that $\lambda_i\geq 4$ is $1$.
  Accordingly, we enter $3$ into the bottom-most boxes in the three leftmost columns, and also into one rightmost box in the first row:
  \begin{displaymath}
    \ytableaushort{{}{}{}{}{}{}3,{}{}3,33}
  \end{displaymath}
  We are left with the problem of finding a semistandard tableau of shape $(6,2)$ and type $(4,4)$.
  Recursively applying our process to this smaller problem gives:
  \begin{displaymath}
    \ytableaushort{{}{}{}{}223,223,33},
  \end{displaymath}
  and finally the desired tableau
  \begin{displaymath}
    \ytableaushort{1111223,223,33}.
  \end{displaymath}
\end{example}
\begin{theorem}
  The complete symmetric functions:
  \begin{displaymath}
    \{h_\mu \mid \text{$\mu$ is a partition of $d$ with at most $n$ parts}\}
  \end{displaymath}
  and the elementary symmetric functions:
  \begin{displaymath}
    \{e_\mu\mid \text{$\mu$ is a partition of $d$ with $\mu_1\leq n$}\}
  \end{displaymath}
  form bases of the space of homogeneous symmetric polynomials of degree $d$ in variables $x_1,\dotsc,x_n$.
\end{theorem}
\begin{proof}
  In view of the triangularity of Kostka numbers (Theorem~\ref{theorem:Kostka-triangularity}) and the fact that $K_{\lambda\lambda}=1$ (Exercise~\ref{exercise:unit-kostka}) the theorem follows from Theorem~\ref{theorem:schur-to-eh}.
\end{proof}
\subsection{The hook-length formula}
\label{sec:hook-length-fmla}
Let $\lambda$ be a partition of $n$.
The $f$-number $f_\lambda = K_{\lambda,(1^n)}$ is given by the hook-length formula:
\begin{equation}
  \label{eq:1hook-length-fmla}
  f_\lambda = \frac{n!}{\prod_{(i,j)\in \lambda} h_{ij}}.
\end{equation}
Here $h_{ij}$ denotes the number of cells in the Young diagram of $\lambda$ that lie either to the right of $(i,j)$ in the $i$th row, or below $(i,j)$ in the $j$th column, including the cell $(i,j)$ itself.
For example, the Young diagram of $\lambda=(6,5,3,3)$ with the hook lengths are entered into the cells is:
\begin{displaymath}
  \ytableaushort{987431,76521,432,321}.
\end{displaymath}
\begin{theorem}
  [Frobenius dimension formula]
  For every partition $\lambda$ of $n$,
  \begin{displaymath}
    f_\lambda = \frac{n!}{\prod_{i=1}^n(\lambda_1 + n - i)!}a_\delta(\lambda_1+n-1,\lambda_2+n-2,\dotsc,\lambda_n).
  \end{displaymath}
\end{theorem}
\begin{proof}
  Work with symmetric polynomials in $n$ variables.
  The starting point of the proof of the Frobenius dimension formula is that $f_\lambda$ is the coefficient of $s_\lambda$ in the elementary symmetric function $e_{(1^n)} = (x_1+\dotsb+x_n)^n$ (by Theorem~\ref{theorem:schur-to-eh}).
  In terms of alternating functions, we have that $f_\lambda$ is the coefficient of $a_{\lambda+\delta}$ in $(x_1+\dotsb+x_n)^na_\delta$.
  This is the same as the coefficients of the strictly decreasing monomial $x^{\lambda+\delta}$ in $(x_1+\dotsb+x_n)^na_\delta$.
  Expanding out the latter sing the multinomial theorem give:
  \begin{displaymath}
    \sum_{\alpha_1+\dotsb+\alpha_n=n} \sum_{w\in S_n} \frac{n!}{\alpha_1!\dotsb \alpha_n!} \epsilon(w) x_1^{\alpha_1+\delta_{w(1)}} x_2^{\alpha_2+\delta_{w(2)}} \dotsb x_n^{\alpha_n+\delta_{w(n)}}.
  \end{displaymath}
  The coefficient of $x^{\lambda+\delta}$ in the above sum is:
  \begin{displaymath}
    \sum_{\alpha, w} \frac{n!}{\alpha_1!\dotsb \alpha_n!}\epsilon(w),
  \end{displaymath}
  the sum being over all vectors $\alpha=(\alpha_1,\dotsc,\alpha_n)$ with non-negative integer coordinates summing to $n$ such that:
  \begin{equation}
    \label{eq:alpha-la}
    \alpha_i + \delta_{w(i)} = \lambda_i + \delta_i.
  \end{equation}
  
  Mutliplying and dividing by the desired denominator of the Frobenius dimension formula gives:
  \begin{align*}
    f_\lambda & = \frac{n!}{\prod_{i=1}^n(\lambda_1 + n - i)!}\sum_{\alpha,w} \epsilon(w) \prod_{i=1}^n \frac{(\lambda_i + \delta_i)!}{\alpha_i!}\\
    & = \frac{n!}{\prod_{i=1}^n(\lambda_1 + n - i)!}\sum_{w\in S_n} \epsilon(w)\prod_{i=1}^n (\lambda_i+\delta_i)(\lambda_i+\delta-1)\dotsb (\lambda_i+\delta_i-\delta_{w(i)} + 1),
  \end{align*}
  where the second step uses the identity (\ref{eq:alpha-la}) relation $\lambda$ and $\alpha$.
  The condition that $\alpha\geq 0$ for each $i$ is now automatically taken care of, since one of the factors in the product
  \begin{displaymath}
    (\lambda_i+\delta_i)(\lambda_i+\delta-1)\dotsb (\lambda_i+\delta_i-\delta_{w(i)} + 1)
  \end{displaymath}
  will be zero if $\alpha_i=\lambda_i+\delta_i-\delta_{w(i)}<0$.

  Setting $f_j(x) = x(x-1)\prod (x-j+1)$, the above expression for $f_\lambda$ becomes the determinant:

\end{proof}
\subsection{Schensted's insertion algorithm}
\label{sec:schenst-insert-algor}
Let $t$ be a semistandard tableau, and $x$ be a positive integer.
Schensted's insertion algorithm is a method of inserting a box with the number $x$ into $t$, resulting in a new tableau $\ins(t, x)$.
Applied repeatedly, it gives a way to convert any word into a tableau.
This tableau succinctly expresses some combinatorial properties of the original word.

First consider the case where $t$ has a single row, with entries $a_1\leq \dotsb \leq a_k$.
Use $\emptyset$ to denote the empty word.
The algorithm $\rins$ takes as input the single row $t$ and a letter $x$, and returns a pair $(b, t')$, where $b'$ is either the empty word, or a single letter, and $t'$ is a row:
\begin{displaymath}
  \rins(a_1a_2\dotsb a_k, x) =
  \begin{cases}
    (\emptyset, a_1\dotsb a_k x) & \text{if $x\geq a_k$},\\
    (a_j, a_1\dotsb a_{j-1} x a_j \dotsb a_k) & \text{ if } j = \min\{r \mid a_r > x\}.
  \end{cases}
\end{displaymath}
In the second case, one says that \emph{$x$ has been inserted into $t=a_1\dotsb a_k$, obtaining $t'=a_1\dotsb a_{j-1} x a_j \dotsb a_k$, and \textbf{bumping out} $a_j$}.
Also, it is notationally convenient to write $\rins(t, \emptyset) = (\emptyset, t)$ (when nothing is inserted, $t$ remains unchanged, and nothing is bumped out).

Now suppose $t$ is a tableau, with rows $r_1,r_2,\dotsc, r_l$.
The reading word of $t$ is $r_l r_{l-1} \dotsb r_1$.
Suppose that $\rins(r_1,x) = (y, r_1')$.
Recursively define:
\begin{displaymath}
  \ins(t, x) = \ins(r_l \dotsb r_2, y) r_1'
\end{displaymath}
\begin{example}
  \label{example:insertion}
  Consider the insertion of $3$ into the tableau:
  \begin{displaymath}
    t = \ytableaushort{13358,2466,358,4}.
  \end{displaymath}
  We have $\rins(13358,3) = (5,13338)$; $\rins(2466,5)=(6,2456)$; $\rins(358,6)=(8,356)$; $\rins(4,8)=(\emptyset,48)$.
  Thus, $\ins(t, 3)$ is the tableau:
  \begin{displaymath}
    \ytableaushort{13338,2456,356,48}.
  \end{displaymath}
In general, it is not possible to recover $t$ and $x$ from $\ins(t,x)$, even if we know $x$.
For example, the above tableau can be obtained by inserting $3$ into a different tableau:
\begin{displaymath}
  \ins\left(\vcenter{\hbox{\ytableaushort{13368,245,356,48}}}, 3\right) = \vcenter{\hbox{\ytableaushort{13338,2456,356,48}}}.
\end{displaymath}
\end{example}
Clearly, the shape of $\ins(t,x)$ can be obtained by adding one box to the shape of $t$.
If we know the row into which the new box was added, and the value of $x$, then $t$ can be recovered from $\ins(t,x)$.
This recovery is based on the fact that $\rins$ can be inverted:
define
\begin{displaymath}
  \rdel(a, a_1a_2\dotsb a_k) = (a_1\dotsb a_{j-1}aa_{j+1}\dotsb a_k, a_j),
\end{displaymath}
where $j=k$ if $a_r\leq a$ for all $r=1,\dotsc, k$ and $j = \min\{r\mid a_{r+1}>a\}$.
To recover $t$ and $x$ from $s=\ins(t,x)$ and $r$, delete the last entry of the $r$th row of $s$, say $x_r$.
Let $u_{r-1}$ denote the $(r-1)$st row of $s$.
Suppose $\rdel(x_r,u_{r-1}) = (v_{r-1},x_{r-1})$, replace the $(r-1)$st row of $s$ with $v_{r-1}$.
Continue this process until $\rdel(x_2,u_1)=(v_1,x_1)$ is obtained and the first row of $s$ is replaced with $v_1$.
The tableau obtained at the end of this process is $t$, and $x=x_1$.
Write $\del(t,r)=(s,x)$.
The preceding discussion shows:
\begin{theorem}
  \label{theorem:del-ins}
  If $\del(t,r)=(s,x)$, then $\ins(s,x)=t$, and $\shape(t)$ is obtained from $\shape(s)$ by adding a cell to its $r$th row.
\end{theorem}
\begin{exercise}
  Verify Theorem~\ref{theorem:del-ins} for the insertions in Example~\ref{example:insertion}.
\end{exercise}
\subsection{Tableaux and Words}
\label{sec:tabl-assoc-word}
Let $A^*_n$ denote the concatenation monoid of all words in the alphabet $\{1,\dotsc,n\}$.
For any $w=a_1\dotsb a_k\in A^*_n$, Schensted's insertion algorithm allows us to associate a unique semistandard tableau $P(w)$ as follows:
\begin{itemize}
\item If $w=a$ has only one letter, then $P(a)$ is the single-cell tableau with entry $a$.
\item If $w=ua$, where $u\in A_n^*$ and $a\in \{1,\dotsc,n\}$, then \linebreak $P(w)=\ins(P(u), a)$.
\end{itemize}
\begin{example}
  If $w=1374433254$, then $P(w)$ is the tableau:
  \begin{displaymath}
    \ytableaushort{12334,345,4,7}
  \end{displaymath}
\end{example}
\label{sec:tableau-as-words}
Given a semistandard tableau $t$, its reading word $w$ is defined to be the sequence of numbers obtained from reading its rows from left to right, starting with the bottom row, and moving up sequentially to the top row.
Since the first entry of each row is strictly smaller than the last entry of the row below it, the tableau $t$ can be recovered from $w$ by chopping it up into segments with a cut after each $a_i$ with $a_{i+1}<a_i$ (we say that $w$ has a descent at $i$). The resulting segments, taken from right to left, form the rows of $t$.
\begin{example}
  The reading word of the tableau $t$ formed at the end of Example~\ref{example:can-tab} is:
  \begin{displaymath}
    w = 332231111223.
  \end{displaymath}
  The tableau $t$ is recovered by marking off the descents $w = 333|223|1111223$, and then rearranging the segments into a tableau.
\end{example}
\begin{exercise}
  \label{exercise:tableau-word}
  Let $w$ denote the reading word of a tableau $t$.
  Show that $P(w)=t$.
\end{exercise}
Not every word comes from a tableau; for example the word $132$, when broken up at descents gives rise to $\ytableaushort{2,13}$.
We shall say that a word is a tableau if it is the reading word of a semistandard tableau.

Call the word $w=a_1\dotsb a_k$ a \emph{row} if $a_1\leq \dotsb \leq a_k$.
Call it a \emph{column} if $a_1>\dotsb>a_k$.
Write $x^w$ for the monomial $x_{a_1}x_{a_2}\dotsb x_{a_k}$.
\begin{exercise}
  \label{exercise:row-col}
  Show that, for every positive integer $i$,
  \begin{displaymath}
    h_k(x_1,\dotsc, x_n) = \sum_{\text{$w\in A_n^*$ is a row of length $k$}}x^w,
  \end{displaymath}
  and 
  \begin{displaymath}
    e_k(x_1,\dotsc, x_n) = \sum_{\text{$w\in A_n^*$ is a column of length $k$}}x^w.
  \end{displaymath}
\end{exercise}
If $w_1$ and $w_2$ are words, and $w_1w_2$ is their concatenation, then
\begin{displaymath}
  x^{w_1}x^{w_2} = x^{w_1w_2}.
\end{displaymath}
This gives rise to an algebra homomorphism called the \emph{evaluation map}:
\begin{displaymath}
\ev:\mathbf Z[A_n^*]\to \mathbf Z[x_1,\dotsc, x_n].
\end{displaymath}
In the algebra $\mathbf Z[A_n^*]$, define elements
\begin{align*}
  \mathbf H_k & = \sum_{\text{$w\in A_n^*$ is a row of length $k$}} w\\
  \mathbf E_k & = \sum_{\text{$w\in A_n*$ is a column of length $k$}} w
\end{align*}
for every positive integer $k$.
Then Exercise~\ref{exercise:row-col} can be restated as the identities:
\begin{displaymath}
  e_k = \ev(\mathbf E_k) \text{ and } h_k = \ev(\mathbf H_k).
\end{displaymath}
The evaluation map has a large kernel; is domain is the free algebra, and it maps onto the polynomial algebra.
The algebra on the right contains our primary object of interest---the algebra $\mathbf Z[x_1,\dotsc,x_n]^{S_n}$ of symmetric polynomials.
In the next few sections, we shall learn about an equivalence relation ``$\equiv$'' on $A_n^*$ such that the resulting quotient monoid $\pl_n:=A_n^*/\equiv$ (called the \emph{plactic monoid}) has the property that the subalgebra of $\mathbf Z[\pl_n]$ generated by the elements $\{\mathbf E_k\}_{k=1}^\infty$ or the elements $\{\mathbf H_k\}_{k=1}^\infty$ is isomorphic to $\mathbf Z[x_1,\dotsc, x_n]^{S_n}$ under the map $w\mapsto x^w$.
\subsection{The Plactic Monoid}
\label{sec:plactic-monoid}
The plactic monoid $\pl(A_n)$ is the quotient of $A^*_n$ by the equivalence relation generated by the Knuth relations:
\begin{gather}
  \tag{$K1$}\label{eq:k1}
  xzy \equiv zxy \text{ if } x\leq y < z,
  \\
  \tag{$K2$}\label{eq:k2}
  yxz \equiv yzx \text{ if } x < y \leq z.
\end{gather}
Two words are said to be in the same plactic class if each can be obtained from the other by a sequence of moves of the form (\ref{eq:k1}) and (\ref{eq:k2}).
Since both sides of the Knuth relations have the same evaluation, it follows that the evaluation map $\ev:\mathbf Z[A^*_n]\to \mathbf Z[x_1,\dotsc,x_n]$ factors through the plactic monoid algebra $\mathbf Z[\pl(A_n)]$.
Let $E_k$ denote the image of $\mathbf E_k$ and $H_k$ denote the image of $\mathbf H_k$ in $\mathbf Z[\pl(A_n)]$.
\begin{exercise}
  Take $n=2$.
  Show that $E_1$ and $E_2$ commute in $\mathbf Z[\pl(A_2)]$.
  Show that they commute in $\mathbf Z[\pl(A_3)]$.
\end{exercise}
\begin{exercise}
  \label{exercise:forgotten}
  Define Sch\"utzenberger's forgotten relations by:
  \begin{gather}
    \tag{$F1$}\label{eq:f1}
    xzy \cong yxz \text{ if } x < y < z,
    \\
    \tag{$F2$}\label{eq:f2}
    zxy \cong yzx \text{ if } x \leq y \leq z.
  \end{gather}
  Let $F(A_n)$ denote the monoid $A_n^*/\cong$.
  Show that the images of $\mathbf E_1$ and $\mathbf E_2$ commute in $F(A_3)$.
\end{exercise}
\begin{exercise}
  Show that any evaluation-preserving equivalence on $A_3^*$ under which the images of $\mathbf E_1$ and $\mathbf E_2$ commute must include either the Knuth equivalences or Sch\"utzenberger's forgotten equivalences.
\end{exercise}
\subsection{The plactic Pieri rules}
Observe that, if for any tableau $t$ and $x\in A_n$, if $t'=\ins(t,x)$, then $\shape(t')$ is obtained by adding one box to $\shape(t)$.
\begin{lemma}
  \label{lemma:two-step}
  Let $t$ be the (the reading word of) a semistandard tableau in $A_n^*$ and $x,y$ be letters in $A_n$.
  Let $t'=\ins(t,x)$ and $t''=\ins(t',y)$.
  Let $a$ be the box added to $\shape(t')$ to obtain $\shape(t'')$, and $b$ be the box added to $\shape(t)$ to obtain $\shape(t')$.
  If $x\leq y$, then $b$ lies in a column strictly to the right of the column of $a$.
  If $x>y$, then $b$ lies in a row strictly below the row of $a$.
\end{lemma}
\begin{proof}
  Let $a_1\dotsb a_k$ be the first row of $t$.
  Consider first the case where $a\leq y$.
  If $a_k\leq x\leq y$, then the result is obvious.
  If $x<a_k$ and $y\geq a_k$, then $b$ lies in the $(k+1)$st column, whereas $x$ bumps some letter $x'\leq y$ to a lower row.
  This letter cannot come to rest in the $(k+1)$st column because that would violate the fact that columns increase strictly in a semistandard tableau.
  If $y<a_k$ as well, then $x$ bumps out $x'$ and $y$ bumps out $y'$ with $x'\leq y'$.
  The problem is now reduced to the tableau obtained by removing the top row of $t$, allowing for the application of induction.
  In the base case (where the original tableau $t$ is a row), $a$ and $b$ are the first and second boxes in the second row of $t''$.

  Now suppose $x>y$.
  If $x\geq a_k$, then $x$ first comes to rest at the end of the first row in $t'$, but then $y$ bumps some element of the first row of $t'$ up to a lower row in $t''$.
  So $a$ lies in the first row and $b$ in a lower row.
  If $x\leq a_k$, then the elements $x'$ and $y'$ bumped out from the first row by $x$ and $y$ respectively again satisfy $x'>y'$, allowing for an inductive argument.
\end{proof}
\begin{theorem}
  [Plactic Pieri rules]
  \label{theorem:plactic-pieri}
  Let $\Tab_n(\lambda)$ denote the set of all semistandard tableaux of shape $\lambda$ and entries in $A_n$.
  Let $R_k(A_n)$ denote the set of rows of length $k$ in $A_n^*$.
  Then the map $(t,r)\mapsto P(tr)$ defines a bijection:
  \begin{displaymath}
    \Tab_n(\lambda)\times R_k(A_n) \to \coprod_{\mu/\lambda \text{ is a horizontal strip of size $k$}} \Tab_n(\mu).
  \end{displaymath}
  Let $C_k(A_n)$ denote the set of columns of length $k$ in $A_n^*$.
  Then the map $(t,c)\mapsto P(tc)$ defines a bijection:
  \begin{displaymath}
    \Tab_n(\lambda)\times R_k(A_n) \to \coprod_{\mu/\lambda \text{ is a vertical strip of size $k$}} \Tab_n(\mu).
  \end{displaymath}
\end{theorem}
\begin{proof}
  Lemma~\ref{lemma:two-step} implies that $\shape(P(wr))$ is obtained from $\shape(P(w))$ by adding a horizontal strip, and that $\shape(P(wc))$ is obtained from \linebreak $\shape(P(w))$ by adding a vertical strip.
  The bijectivity can be shown by repeated used of the $\del$ algorithm (because we know which box from $\mu$ has to be removed at each step).
\end{proof}
\begin{corollary}
  [Kostka's definition of Schur functions]
  \label{corollary:kostka-def-schur}
  For every partition $\lambda$ and every positive integer $n$, we have:
  \begin{displaymath}
    s_\lambda(x_1,\dotsc,x_n) = \sum_{t\in\Tab_n(\lambda)} x^t.
  \end{displaymath}
\end{corollary}
\begin{proof}
  Define $S_\lambda = \sum_{t\in \Tab_n(\lambda)} x^t$.
  Then $S_{(n)} = h_n$, and $S_{(1^n)} = e_n$, just like the Schur functions.
  Moreover, the class of functions $S_\lambda$ satisfy the Pieri rule, just like the Schur function.
  This alone is enough to imply that the identities of Theorem~\ref{theorem:schur-to-eh} hold with $s_\lambda$ replaced by $S_\lambda$ (since the proof only uses properties that are common to $s_\lambda$ and $S_\lambda$.
  By the triangularity properties of of Kostka numbers described in Section~\ref{sec:triang-kostka-numb}, these identities uniquely determine the Schur functions, therefore $S_\lambda=s_\lambda$ for every partition $\lambda$.
\end{proof}
\subsection{The Lindstr\"om-Gessel-Viennot Lemma}
\label{sec:lgv}
Let $R$ be a commutative ring.
Let $S$ be any set of points, and $v:S\times S\to R$ be any function (we think of $w$ as a \emph{weight function}.
Given $s, t\in S$, a path in $S$ from $s$ to $t$ is is a sequence $\omega=(s=s_0,s_1,\dotsc,s_k=t)$ of distinct points in $S$.
We denote this by $\omega:s\to t$.
The weight of the path $\omega$ is defined to be:
\begin{displaymath}
  v(\omega) = v(s_0,s_1)v(s_1,s_2)\dotsb v(s_{k-1}, s_k).
\end{displaymath}
\begin{definition}
  [Crossing paths]
  Two paths $\omega=(s_0,\dotsc, s_k)$ and $\eta=(t_0,\dotsc,t_l)$ are said to cross if $s_i= t_j$ for some $0\leq i \leq k$ and $0\leq j \leq l$.
\end{definition}
\begin{definition}
  [Crossing condition]
  \label{definition:crossing-condition}
  Given a set $S$ of points, a weight function $v:S\times S\to R$, and a points $A_1,\dotsc,A_n$, and $B_1,\dotsc, B_n$, we say that the \emph{crossing condition} is satisfied if, whenever $1\leq i<j\leq n$ and $1\leq i'<j'\leq n$, and $\omega:i \to j'$ and $\eta: j\to i'$ are paths such that $v(\omega)\neq 0$ and $v(\eta)\neq 0$, then the paths $\omega$ and $\eta$ cross.
\end{definition}
Fix points $A_1,\dotsc, A_n$ and $B_1,\dotsc, B_n$ in $S$, and define an $n\times n$ matrix $(a_{ij})$ by:
\begin{displaymath}
  a_{ij} = \sum_{\omega:A_i\to B_j} v(\omega).
\end{displaymath}
\begin{theorem}
  [Lindstr\"om-Gessel-Viennot Lemma]
  \label{lemma:lgv}
  Assume that the crossing condition (Definition~\ref{definition:crossing-condition}) holds.
  Then the determinant of the matrix $(a_{ij})$ defined above is given by:
  \begin{equation}
    \label{eq:lgv}
    \det(a_{ij}) = \sum_{\omega_i:A_i\to B_i} v(\omega_1)\dotsb v(\omega_n),
  \end{equation}
  where the sum is over all $n$-tuples $(\omega_1,\dotsc, \omega_n)$ of pairwise non-crossing paths $\omega_i:A_i\to B_i$.
\end{theorem}
\begin{proof}
  Let $P$ be the set of all $n$-tuples of paths of the form:
  \begin{equation}
    \label{eq:paths}
    \bar\omega = (\omega_i:A_i\to B_{w(i)}, i=1,\dotsc, n),
  \end{equation}
  where $w$ is a permutation of $\{1,\dotsc, n\}$.
  Define the weight of $\bar \omega\in P$ by:
  \begin{displaymath}
    v(\bar\omega) = \prod_{i=1}^n v(\omega_i)
  \end{displaymath}
  and its sign by $\epsilon(\bar\omega)=\epsilon(w)$.
  Then the determinant on the left hand side of (\ref{eq:lgv}) expands to the sum:
  \begin{equation}
    \label{eq:det-expansion}
    \sum_{\bar\omega\in P} \epsilon(\bar\omega)v(\bar\omega).
  \end{equation}
  Define an involution $I:P\to P$ as follows: given $\bar\omega$ as in (\ref{eq:paths}), if the paths are pairwise non-crossing, then $\bar\omega$ is a fixed point for $I$.
  In this case the crossing condition implies that $w$ is the identity permutation.
  Otherwise, take the least $i$ such that the path $\omega_i=(s_0,\dotsc,s_k)$ crosses another path $\omega_j=(t_0,\dotsc,t_l)$.
  Let $j$ be the smallest number such that a point $s_j$ of $\omega_i$ lies in the path $\omega_j$, say $s_j=t_r$.
  Let $I(\bar\omega)$ be the family of paths obtained from $\bar\omega$ by modifying $\omega_i$ and $\omega_j$ to $\omega'_i$ and $\omega'_j$ as follows:
  \begin{align*}
    \omega'_i & = (s_0,\dotsc, s_j, t_{r+1}, \dotsc, t_l),\\
    \omega'_j & = (t_0,\dotsc, t_r, s_{j+1}, \dotsc, s_k).
  \end{align*}
  Clearly, $v(I(\bar\omega)) = v(\bar\omega)$ and $\epsilon(I(\bar\omega)) = - \epsilon(\bar\omega)$.
  This involution cancels out all the terms in (\ref{eq:det-expansion}) except those that occur on the right hand side of (\ref{eq:lgv}).
\end{proof}
\subsection{The Jacobi-Trudi Identities}
\label{sec:jacobi-trudi-ident}
We have seen that the Kostka numbers can be used to express complete and elementary symmetric functions in terms of Schur functions.
The reverse operation---that of expressing Schur functions in terms of complete or elementary symmetric functions---is done by the Jacobi-Trudi identities:
\begin{theorem}
  [Jacobi-Trudi identities]
  For every integer partition $\lambda=(\lambda_1,\dotsc,\lambda_l)$ form the $l\times l$ matrices with $(i,j)$th entry $h_{\lambda_i-i+j}$ and $e_{\lambda_i-i+j}$ respectively.
  Then
  \begin{displaymath}
    s_\lambda = \det(h_{\lambda_i-i+j}), \quad s_{\lambda'} = \det(e_{\lambda_i-i+j}).
  \end{displaymath}
\end{theorem}
\begin{proof}
  The Jacobi-Trudi identities can be proved using the Lindstr\"om-Gessel-Viennot lemma (Theorem~\ref{lemma:lgv}).
  For the first identity take $S$ to be the positive cone in the the integer lattice:
  \begin{displaymath}
    S = \{(i,j)\mid i\geq 0,\; j>0 \text{ are integers}\}.
  \end{displaymath}
  Set the weight $v((i,j), (i+1,j))$ of each rightward horizontal edge to be $x_j$ for $j=1,\dotsc, n$, the weight of each upward vertical edge $v((i,j), (i,j+1))$ to be $1$ for all $j=1,\dotsc,n-1$.
  The remaining weights are all zero.
  \begin{lemma}
    \label{lemma:entry}
    For all integers $i>0$ and $j\geq 0$, we have:
    \begin{displaymath}
      \sum_{\omega:(i, 1)\to (i+j, n)} v(\omega) = h_j(x_1,\dotsc,x_n).
    \end{displaymath}
  \end{lemma}
  \begin{proof}
    Only rightward or upward steps have non-zero weights.
    So every path with non-zero weight is composed of unit upward and rightward steps.
    A path with non-zero weight from $(i, 1)$ to $(i+j, n)$ must have exactly $j$ rightward steps, say in rows $1\leq i_1\geq i_2 \dotsb \leq i_j\leq n$.
    The weight of such a path is $x_{i_1}\dotsb x_{i_j}$, and hence, the sum of the weights of all such paths is $h_j(x_1,\dotsc, x_n)$. For an example, see Fig.~\ref{fig:path_example}.
  \end{proof}
  \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{path_example.png}
    \caption{A path from $(0, 5)$ to $(6, 1)$ whose weight is the monomial $x_1^2x_3^3x_5$ in $h_6(x_1,\dotsc,x_5)$.}
    \label{fig:path_example}
  \end{figure}
  Given $\lambda=(\lambda_1,\dotsc,\lambda_l)$, and working with $n$ variables $x_1,\dotsc,x_n$, let $A_i = (l-i, 1)$ and $B_i=(\lambda_i+l-i, n)$ for $i=1,\dotsc, l$.
  Then  by Lemma~\ref{lemma:entry},
  \begin{displaymath}
    \sum_{\omega:A_i\to B_j} v(\omega) = h_{\lambda_j+i-j}.
  \end{displaymath}
  \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{jth.png}
    \caption{Non-crossing paths corresponding to the tableau \small{$\ytableaushort{113,24,35,5}$}}
    \label{fig:jacobi-trudi}
  \end{figure}
  So the left-hand-side of the first Jacobi-Trudi identity is the left-hand-side of the Lindstr\"om-Gessel-Viennot lemma.
  The right hand side of the Linstr\"om-Gessel-Viennot lemma consists of a sequence of non-crossing paths $(\omega_1,\dotsc,\omega_n)$, where $\omega_i:A_i\to B_i$.
  Reading the row numbers of the horizontal steps in $\omega_i$ gives a weakly increasing sequence of integers $1\leq k_1 \leq \dotsb \leq k_{\lambda_i}\leq n$.
  Enter these numbers into the $i$th row of the Young diagram of $\lambda$ for $i=1,\dotsc,n$.
  Since the paths are non-crossing, the $j$th rightward step of $\omega_i$ must be strictly higher than the $j$th rightward step of $\omega_{i+1}$.
  This means that the columns of the resulting numbering are strictly increasing, resulting in a semistandard tableau of shape $\lambda$.

  Figure~\ref{fig:jacobi-trudi} shows the non-crossing path configuration corresponding to $n=5$, $\lambda=(3,2,2,1)$ which corresponds to the semistandard tableau \small{$\ytableaushort{113,24,35,5}$}.
  Thus, it follows from the Lindstr\"om-Gessel-Viennot lemma that
  \begin{displaymath}
    \det(h_{\lambda_j+i-j}) = \sum_{t\in \tab(\lambda)} x^t.
  \end{displaymath}

  For the second Jacobi-Trudi identity take
  \begin{displaymath}
    S = \{(i,j)\mid i\geq 0,\;j\geq 0\}.
  \end{displaymath}
  Define $v((i,j),(i+1,j))=1$ (as before) and $v((i-1,j),(i,j+1))=x_i$; all other weights are zero.
  For the new weights, the analog of Lemma~\ref{lemma:entry} is:
  \begin{lemma}
    \label{lemma:entry-e}
    For all integers $i>0$ and $j>0$, we have:
    \begin{displaymath}
      \sum_{\omega:(i,0)\to (i+j,n)} v(\omega) = e_j(x_1,\dotsc,x_n).
    \end{displaymath}
  \end{lemma}
  \begin{proof}
    Every path with non-zero weights consists of unit upward or upper-rightward diagonal steps.
    A path with non-zero weight from $(i,0)$ to $(i+j,n)$ must have $n$ such steps, of which $j$ must be diagonal.
    If the steps numbered $i_1,\dotsc,i_j$ are the diagonal steps, then the path has weight $x_{i_1}\dotsb x_{i_j}$.
    For an example of such a path, see Fig.~\ref{fig:example_path_e}.
    Summing over all possible paths gives $e_j(x_1,\dotsc,x_n)$.
  \end{proof}
  \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{path_example2.png}
    \caption{A path from $(2,0)$ to $(4,5)$ whose weight is the monomial $x_1x_4$ in $e_2(x_1,\dotsc,x_5)$.}
    \label{fig:example_path_e}
  \end{figure}
  Suppose that the conjugate partition of $\lambda$ is $\lambda'=(\lambda'_1,\dotsc,\lambda'_k)$.
  In order to apply the Lindstr\"om-Gessel-Viennot lemma to obtain the second Jacobi-Trudi identity, take $A_i=(k-i, 0)$ and $B_i=(\lambda'_i+k-i, n)$ for $i=1,\dotsc,k$.
  Then by Lemma~\ref{lemma:entry-e},
  \begin{displaymath}
    \sum_{\omega_i:A_i\to B_j} v(\omega) = e_{\lambda_j+i-j}.
  \end{displaymath}
  So the left-hand-side of the second Jacobi-Trudi identity is the left-hand-side of the Lindstr\"om-Gessel-Viennot lemma.

  The right hand side of the Linstr\"om-Gessel-Viennot lemma consists of a sequence of non-crossing paths $(\omega_1,\dotsc,\omega_n)$, where $\omega_i:A_i\to B_i$.
  Reading the row numbers where the upper-rightward steps in $\omega_i$ originate gives a strictly increasing sequence of integers $1\leq k_1 \leq \dotsb \leq k_{\lambda'_i}\leq n$.
  Enter these numbers into the $i$th column of the Young diagram of $\lambda$.
  Since the paths are non-crossing, the $j$th upper-rightward step of $\omega_i$ must be no lower than the $j$th upper-rightward step of $\omega_{i+1}$.
  This means that the rows of the resulting numbering are weakly increasing, resulting in a semistandard tableau of shape $\lambda$.
  \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{jte.png}
    \caption{Non-crossing paths corresponding to the tableau \small{$\ytableaushort{113,24,35,5}$}.}
    \label{fig:jte}
  \end{figure}
  Figure~\ref{fig:jacobi-trudi} shows the non-crossing path configuration corresponding to $n=5$, $\lambda=(3,2,2,1)$ which corresponds to the semistandard tableau \small{$\ytableaushort{113,24,35,5}$}.
  Thus, it follows from the Lindstr\"om-Gessel-Viennot lemma that
  \begin{displaymath}
    \det(e_{\lambda_j+i-j}) = \sum_{t\in \tab(\lambda')} x^t,
  \end{displaymath}
  proving the second Jacobi-Trudi identity.
\end{proof}
\subsection{Giambelli's identity}
\label{sec:giambelli}
\begin{definition}
  [Frobenius coordinates]
  Let $\lambda=(\lambda_1,\dotsc,\lambda_l)$ be a partition.
  Its \emph{Drufee rank} $d$ is defined to be the largest integer $i$ such that $(i,i)$ lies in the Young diagram of $\lambda$.
  Let $\alpha_i$ denote the number of cells in the $i$th row that lie strictly to the right of $(i,i)$ in the Young diagram of $\lambda$.
  Similarly let $\beta_i$ denote the number of cells in the $i$th column that lie strictly below $(i,i)$.
  Clearly $\alpha_1>\dotsb>\alpha_d$, $\beta_1>\dotsb>\beta_d)$, and the Young diagram of $\lambda$ can be recovered from the data $(\alpha|\beta)=(\alpha_1,\dotsc,\alpha_d|\beta_1,\dotsc,\beta_d)$, which are called the \emph{Frobenius coordinates} of $\lambda$\footnote{While constructing the character tables of symmetric groups, Frobenius used these coordinates to index the irreducible representation, while he used the ordinary coordinates to index the congugacy classes.}.
\end{definition}
\begin{example}
  The hook partition $(a+1,1^b)$ has Frobenius coordinates $(a|b)$.
  Hook partitions are precisely those partitions which have Durfee rank $1$.
  The partition with frobenius coordinates $(5,2,1|4,3,0)$ is $(6,4,4,2,2)$.
  If $\lambda$ has Frobenius coordinates $(\alpha|\beta)$, then its conjugate $\lambda'$ has Frobenius coordinates $(\beta|\alpha)$.
  The size of a partition with Durfee rank $d$ and Frobenius cooedinates $(\alpha|\beta)$ is $d+|\alpha|+|\beta|$. 
\end{example}
Schur functions of hook partitions can be calculated using Exercise~\ref{exercise:hook-schur}, which, when written is terms of Frobenius coordinates, becomes:
\begin{equation}
  \label{eq:hook-schur-frob}
  s_{(a|b)} = \sum_{l=0}^b (-1)^l h_{a+l+1}e_{b-l}
\end{equation}
\begin{theorem}
  [Giambelli's formula]
  For a partition $(\alpha_1,\dotsc,\alpha_d|\beta_1,\dotsc,\beta_d)$  in Frobenius coordinates,
  \begin{equation}
    \label{eq:giambelli}
    s_{(\alpha|\beta)} = \det(s_{(\alpha_i|\beta_j)})_{d\times d}.
  \end{equation}
  Note that the determinant on the right consists of hook-partition Schur functions, which are given by \textup{(\ref{eq:hook-schur-frob})}.
\end{theorem}
\begin{example}
  The Schur function for $\lambda=(6,4,4,2,2)$ can be computed as:
  \begin{displaymath}
    s_{(5,2,1|4,3,0)} = \det
    \begin{pmatrix}
      s_{(5|4)} & s_{(5|3)} & s_{(5|0)}\\
      s_{(2|4)} & s_{(2|3)} & s_{(2|0)}\\
      s_{(1|4)} & s_{(1|3)} & s_{(1|0)}\\
    \end{pmatrix}
  \end{displaymath}
\end{example}
\begin{proof}
  The determinant on the right hand side of (\ref{eq:giambelli}) can be written as:
  \begin{equation}
    \label{eq:giambelli-det}
    \sum_{w\in S_d} \epsilon(w) \prod_{t^i\in \Tab(\alpha_i,\beta_{w(i)})} x^{t_i}.
  \end{equation}
  Let $P$ be the set of all $d$-tuples of semistandard hook-shaped tableaux $t^1,\dotsc,t^d$, where $t^i\in \Tab(\alpha_i,\beta_{w(i)})$.
  Then we can assemble $(t^1,\dotsc,t^d)$ into a tableau $t$ of shape $(\alpha|\beta)$ as follows:
  The entries in the arm of $t^i$ go into the $i$th arm of $t$.
  The corner entry of $t^i$ goes into the $(w(i),w(i))$th node of $t$.
  The entries in the leg of $t_i$ go into the $w(i)$th leg of $t$.
\end{proof}
\subsection{The Robinson-Schensted-Knuth Correspondences}
\label{sec:rsk}
For an $m\times n$ matrix $A=(a_{ij})$, the column word $u_A$, row word $v_A$ and their duals $\bar u_A$ and $\bar v_A$ are defined as follows:
\begin{align*}
  u_A &= 1^{a_{11}}2^{a_{12}}\dotsb n^{a_{1n}} \; 1^{a_{21}} 2^{a_{22}} \dotsb n^{a_{2n}} \; \dotsb \; 1^{a_{m1}} 2^{a_{m2}} \dotsb n^{a_{mn}}\\
  v_A &= 1^{a_{11}}2^{a_{21}}\dotsb m^{a_{m1}} \; 1^{a_{12}} 2^{a_{22}} \dotsb m^{a_{m2}} \; \dotsb \; 1^{a_{1n}} 2^{a_{2n}} \dotsb m^{a_{mn}}\\
  \bar u_A &= n^{a_{1n}} \dotsb 2^{a_{12}}1^{a_{11}}\; n^{a_{2n}}\dotsb 2^{a_{22}}1^{a_{21}} \; n^{a_{mn}} \dotsb 2^{a_{m2}} 1^{a_{11}}\\
  \bar v _A &= m^{a_{m1}}\dotsb 2^{a_{21}}1^{a_{11}} \; m^{a_{m2}}\dotsb 2^{a_{22}} 1^{a_{12}} \; \dotsb \; m^{a_{mn}}\dotsb  2^{a_{2n}} \dotsb 1^{a_{1n}}
\end{align*}
\begin{definition}
  [Robinson-Schensted-Knuth Correspondences]
  Define functions from integer matrices onto pairs of semistandard tableaux by:
  \begin{align*}
    \rsk(A) & = (P(u_A), P(v_A)),\\
    \rsk^*(A) & = (P^*(u_A), P(\bar v_A)).
  \end{align*}
\end{definition}
\begin{lemma}
  For every integer matrix $A$ with non-negative entries, the tableaux $P(u_A)$ and $P(v_A)$ have the same shape.
  For every zero-one matrix $A$, the tableaux $P^*(u_A)$ and $P(\bar v_A)$ have the same shape.
\end{lemma}
\begin{proof}
  This follows from Greene's theorem.
\end{proof}
\begin{theorem}
  [Knuth's theorem]
  \label{theorem:knuth}
  Let $\mathbf M_{\mu\nu}$ denote the set of integer matrices with non-negative entries, row sums $(\mu_1,\dotsc, \mu_m)$, column sums $(\nu_1,\dotsc,\nu_n)$.
  Then $\rsk$ gives rise to a bijection:
  \begin{displaymath}
    \mathbf M_{\mu\nu}\tilde\to \coprod_\lambda \Tab(\lambda,\nu)\times \Tab(\lambda,\mu).
  \end{displaymath}
  Similarly let $\mathbf N_{\mu\nu}$ denote the set of zero-one matrices with row sums $(\mu_1,\dotsc, \mu_m)$ and column sums $(\nu_1,\dotsc,\nu_n)$.
  Then $\rsk^*$ gives rise to a bijection:
  \begin{displaymath}
    \mathbf N_{\mu\nu}\tilde\to \coprod_\lambda \Tab(\lambda',\nu)\times \Tab(\lambda,\mu).
  \end{displaymath}
\end{theorem}
\begin{exercise}
  [The Burge Correspondence]
  Define
  \begin{displaymath}
    \bur(A) = (P^*(\bar u_A), P^*(\bar v_A)).
  \end{displaymath}
  Show that $\bur$ is a bijection
  \begin{displaymath}
    \mathbf M_{\mu\nu}\tilde \to \coprod_\lambda \Tab^*(\lambda,\nu)\times \Tab^*(\lambda,\mu),
  \end{displaymath}
  where $\Tab^*(\lambda,\mu)$ denotes the set of tableaux of shape $\lambda$ and type $\mu$ with strictly increasing columns, and weakly increasing rows.
\end{exercise}
\subsection{The Littlewood-Richardson Rule}
\label{sec:littlewood-richardson}
\begin{lemma}
  \label{lemma:section}
  Given a partition $\lambda$, fix any $t_\lambda\in \Tab_m(\lambda)$.
  Then
  \begin{displaymath}
    \sum_{\{A_{m\times n}\mid P(v_A) = t_\lambda\}} x^{u_A} = s_\lambda(x_1,\dotsc,x_n).
  \end{displaymath}
\end{lemma}
\begin{proof}
  If $P(v_A) = t_\lambda$, a tableau of shape $\lambda$, $P(u_A)$ is also a semistandard tableau of shape $\lambda$.
  Moreover, for every semistandard tableau $t\in \Tab_n(\lambda)$,  by Knuth's theorem (Theorem~\ref{theorem:knuth}), there exists a unique $m\times n$ integer matrix $A$ such that $\rsk(A) = (t, t_\lambda)$.
  In other words, among matrices with $P(v_A)=t_\lambda$, there exists a unique matrix such that $u_A\equiv t$.
  The lemma now follows from Kostka's definition of Schur functions (Corollary~\ref{corollary:kostka-def-schur}).
\end{proof}
Given a partition $\lambda$, let $t^0_\lambda$ denote the unique semistandard tableau of shape $\lambda$ and type $\lambda$.
All the boxes in the $i$th row of this tableau are filled with the integer $i$.
\begin{theorem}
  [Littlewood-Richardson Rule]
  Let $\alpha$, $\beta$ and $\lambda$ be partitions.
  Let $c^\lambda_{\alpha\beta}$ denote the number of semistandard skew-tableaux of shape $\lambda/\alpha$ whose reading word is Knuth-equivalent to $t^0_\beta$, the unique tableau of shape and type $\beta$.
  Then
  \begin{displaymath}
    s_\alpha s_\beta = \sum_\lambda c^\lambda_{\alpha\beta} s_\lambda.
  \end{displaymath}
\end{theorem}
\begin{proof}
  By Lemma~\ref{lemma:section}, we have:
  \begin{align}
    \nonumber s_\alpha s_\beta & = \sum_{\{A_{a\times n}\mid P(v_{A'}) = t_\alpha\}} x^{u_{A'}} \sum_{\{B_{b\times n}\mid P(v_B) = t_\beta} x^{u_{A''}}\\
    \label{eq:sum-C} & = \sum_C x^{u_C},
  \end{align}
  where $C$ runs over all $(a+b)\times n$ block matrices of the form $\binom AB$, with $P(v_A)=t_\alpha$ and $P(v_B)=t_\beta$.
  \begin{lemma}
    Given integer matrices $A_{a\times n}$ and $B_{b\times n}$, form the block matrix $C_{(a+b)\times n} = \binom AB$.
    Given a word in $L_{a+b}$, let $w^a$ denote the subword of $a$ obtained by removing all letters of $w$ that do not lie in $[1, a]$.
    Let $w^b$ denote the word obtained from $w$ by removing all letters of $w$ that lie in $[1,a]$, and then replacing each of the remaining letters $x$ (which will lie in $[a+1, a+b]$) by $x-a$ (so that they now lie in $[1, b]$).
    Then $P(v_A) = t_\alpha$ and $P(v_B) = t_\beta$ if and only if $P(v_C)^a = t_\alpha$, and $P(v_C)^b \equiv t_\beta$.
  \end{lemma}
  Thus the sum 
  Given a word $w\in A_n^*$, and a subset $S\subset A_n$, let $w^S$ denote the word obtained by deleting all the letter from $w$ that are not in $S$.
  \begin{lemma}
    Let 
    \begin{displaymath}
      C(\alpha,\beta) = \{t\in \Tab(A_{a+b}) \mid t^{[0, a]} = t^0_\alpha,\;t^{[a+1, a+b]} \equiv t^0_\beta\}.
    \end{displaymath}
    Taking $A = \binom{A'}{A''}$ to $(A', A'')$ gives rise a bijection:
    \begin{displaymath}
      \{A\mid P(v_A) \in C(\alpha,\beta)\} \tilde\to \{(A', A'')\mid P(v_{A'}) = t^0_\alpha,\; P(v_{A''}) = t^0_\beta\}.
    \end{displaymath}
  \end{lemma}
  Thus, as $A'$ and $A''$ run over all matrices with $P(v_{A'})=t^0_\alpha$ and $P(v_{A''})=t^0_\beta$, $A=\binom{A'}{A''}$ runs over all matrices with $P(v_A)\in C(\alpha,\beta)$.
  It follows that
  \begin{displaymath}
    s_\alpha s_\beta = \#\{t\in \Tab(A_{a+b})\mid t^{[0,a]} = t^0_\alpha,\; t^{[a+1,b]}\equiv t^0_\beta\}.
  \end{displaymath}
  Ignoring the entries of the boxes of $t$ which lie in $\alpha$ (since they are fixed) we get a semistandard tableau $s$ of shape $\lambda/\alpha$.
  The condition that $t^{[a+1,b]}\equiv t^0_\beta$ is equivalent to saying that the reading word of $s$ lies in the plactic class of $t^0_\beta$.
\end{proof}
\begin{definition}
  [Yamanouchi Word]
  A word $w\in A_n^*$ is called a Yamanouchi word if $x^u$ is a monomial with weakly increasing powers for every suffix $u$ of $w$.
\end{definition}
\begin{lemma}
  \label{lemma:yamanouchi}
  A word $w$ is a Yamanouchi word of type $\lambda$ if and only if its plactic class contains the unique semistandard tableau $t^0_\lambda$ of shape $\lambda$ and type $\lambda$.
\end{lemma}
\begin{proof}
  Check that Knuth relations preserve \emph{Yamanouchiness}.
  The only Yamanouchi tableau of type $\lambda$ also has shape $\lambda$.
\end{proof}
In view of Lemma~\ref{lemma:yamanouchi}, we may restate the Littlewood-Richardson rule as:
\begin{theorem}
  For partition $\alpha,\beta,\lambda$, $c^\lambda_{\alpha\beta}$ is the number of semistandard tableau of shape $\lambda/\alpha$ and type $\beta$ whose reading word is a Yamanouchi word.
\end{theorem}
\bibliographystyle{abbrv}
\bibliography{refs}
\end{document}

